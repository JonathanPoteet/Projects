{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvG/YFgJyoXx2dx+72W0Mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VU-cs5891/assignment-3-JonathanPoteet/blob/main/Special_Topics_AI_Predictive_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3: Create a Predictive Model"
      ],
      "metadata": {
        "id": "3NNWH8cZgza6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment focuses on building a predictive model and conducting a subsequent bias analysis using a selected biased dataset. The primary goal is to predict a specific outcome based on demographic data and then analyze any potential biases present in the model's performance due to the characteristics of the data.\n",
        "\n",
        "To handle this I took these steps:\n",
        "\n",
        "1.  **Data Loading and Initial Analysis:** Loading the dataset and performing an initial examination of its structure and content.\n",
        "2.  **Data Cleaning and Preprocessing:** Cleaning the data to handle missing values and prepare it for modeling. This includes addressing specific data irregularities identified in relevant features.\n",
        "3.  **Predictive Model Creation:** Developing a predictive model, specifically a Random Forest Classifier, which is a suitable binary classification model for this task.\n",
        "4.  **Model Evaluation and Bias Analysis:** Evaluating the performance of the trained model using appropriate metrics and analyzing potential biases in its predictions, considering the known characteristics and potential limitations of the dataset."
      ],
      "metadata": {
        "id": "wevO58R7pHYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading, Imports, and Initial Analysis:"
      ],
      "metadata": {
        "id": "5VvkEV8tqeUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin with by accessing the data:"
      ],
      "metadata": {
        "id": "f8ezlAQ1rvGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB_l8yIez0oG",
        "outputId": "e6c00d8b-e296-4d74-c162-a0f55d4400b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/My\\ Drive/Vanderbilt\\ Class\\ Misc/\"Special Topics AI\"\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uztPtDJn-b4",
        "outputId": "99b88fab-c9bc-4290-aefd-ec8cdb289c6f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Vanderbilt Class Misc/Special Topics AI\n",
            "'Assignment 1 Special Topics AI.gdoc'   biased_demographic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing tools:"
      ],
      "metadata": {
        "id": "8dddewOy30io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "XNUCduDgokbn"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data of this dataset is provided in codes, so in order to properly understand and work with it, I am using a data dictionary. This data dictionary is based on the 2013-2014 Demographics Variable List provided by the Centers for Disease Control and Prevention (CDC) through the National Health and Nutrition Examination Survey (NHANES). The variables listed below correspond to the columns found in the 2013-2014 demographic dataset (demographic.csv) available from Kaggle and the CDC.\n",
        "\n",
        "The dataset can be found here: https://www.kaggle.com/datasets/cdc/national-health-and-nutrition-examination-survey/data?select=demographic.csv\n",
        "\n",
        "The variable names and definitions were obtained from the official CDC NHANES documentation, accessible at:\n",
        "https://wwwn.cdc.gov/Nchs/Nhanes/Search/variablelist.aspx?Component=Demographics&CycleBeginYear=2013"
      ],
      "metadata": {
        "id": "gkagMk3S0jPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_data = [\n",
        "    {'Code': 'SEQN', 'Definition': 'Respondent sequence number'},\n",
        "    {'Code': 'SDDSRVYR', 'Definition': 'Data release cycle'},\n",
        "    {'Code': 'RIDSTATR', 'Definition': 'Interview and examination status of the participant'},\n",
        "    {'Code': 'RIAGENDR', 'Definition': 'Gender of the participant'},\n",
        "    {'Code': 'RIDAGEYR', 'Definition': 'Age in years at screening (topcoded at 80)'},\n",
        "    {'Code': 'RIDAGEMN', 'Definition': 'Age in months at screening (for participants aged 24 months or younger)'},\n",
        "    {'Code': 'RIDRETH1', 'Definition': 'Race/Hispanic origin (Recode)'},\n",
        "    {'Code': 'RIDRETH3', 'Definition': 'Race/Hispanic origin with Non-Hispanic Asian category'},\n",
        "    {'Code': 'RIDEXMON', 'Definition': 'Six-month time period when the examination was performed'},\n",
        "    {'Code': 'RIDEXAGM', 'Definition': 'Age in months at examination (for participants aged 19 years or younger)'},\n",
        "    {'Code': 'DMQMILIZ', 'Definition': 'Ever served on active duty in U.S. Armed Forces, Reserves, or National Guard'},\n",
        "    {'Code': 'DMQADFC', 'Definition': 'Ever served in a foreign country during armed conflict or on a humanitarian mission'},\n",
        "    {'Code': 'DMDBORN4', 'Definition': 'Country of birth'},\n",
        "    {'Code': 'DMDCITZN', 'Definition': 'Citizenship status'},\n",
        "    {'Code': 'DMDYRSUS', 'Definition': 'Length of time in the U.S.'},\n",
        "    {'Code': 'DMDEDUC3', 'Definition': 'Education level for participants aged 6–19 years'},\n",
        "    {'Code': 'DMDEDUC2', 'Definition': 'Education level for participants aged 20 years and older'},\n",
        "    {'Code': 'DMDMARTL', 'Definition': 'Marital status'},\n",
        "    {'Code': 'RIDEXPRG', 'Definition': 'Pregnancy status at MEC exam (for females aged 20–44 years)'},\n",
        "    {'Code': 'SIALANG', 'Definition': 'Language of the Sample Person interview'},\n",
        "    {'Code': 'SIAPROXY', 'Definition': 'Was a proxy respondent used in the Sample Person interview?'},\n",
        "    {'Code': 'SIAINTRP', 'Definition': 'Was an interpreter used in the Sample Person interview?'},\n",
        "    {'Code': 'FIALANG', 'Definition': 'Language of the Family interview'},\n",
        "    {'Code': 'FIAPROXY', 'Definition': 'Was a proxy respondent used in the Family interview?'},\n",
        "    {'Code': 'FIAINTRP', 'Definition': 'Was an interpreter used in the Family interview?'},\n",
        "    {'Code': 'MIALANG', 'Definition': 'Language of the MEC interview'},\n",
        "    {'Code': 'MIAPROXY', 'Definition': 'Was a proxy respondent used in the MEC interview?'},\n",
        "    {'Code': 'MIAINTRP', 'Definition': 'Was an interpreter used in the MEC interview?'},\n",
        "    {'Code': 'AIALANGA', 'Definition': 'Language of the Audio Computer-Assisted Self-Interview (ACASI)'},\n",
        "    {'Code': 'DMDHHSIZ', 'Definition': 'Total number of people in the household'},\n",
        "    {'Code': 'DMDHHSZA', 'Definition': 'Number of children aged 0–5 years in the household'},\n",
        "    {'Code': 'DMDHHSZB', 'Definition': 'Number of children aged 6–17 years in the household'},\n",
        "    {'Code': 'DMDHHSZE', 'Definition': 'Number of adults aged 60 years or older in the household'},\n",
        "    {'Code': 'DMDHRGND', 'Definition': 'Gender of the household reference person'},\n",
        "    {'Code': 'DMDHRAGE', 'Definition': 'Age of the household reference person'},\n",
        "    {'Code': 'DMDHRBR4', 'Definition': 'Country of birth of the household reference person'},\n",
        "    {'Code': 'DMDHREDU', 'Definition': 'Education level of the household reference person'},\n",
        "    {'Code': 'DMDHRMAR', 'Definition': 'Marital status of the household reference person'},\n",
        "    {'Code': 'DMDHSEDU', 'Definition': 'Education level of the spouse of the household reference person'},\n",
        "    {'Code': 'WTINT2YR', 'Definition': 'Full sample 2-year interview weight'},\n",
        "    {'Code': 'WTMEC2YR', 'Definition': 'Full sample 2-year MEC exam weight'},\n",
        "    {'Code': 'SDMVPSU', 'Definition': 'Masked variance unit pseudo-PSU variable for variance estimation'},\n",
        "    {'Code': 'SDMVSTRA', 'Definition': 'Masked variance unit pseudo-stratum variable for variance estimation'},\n",
        "    {'Code': 'INDHHIN2', 'Definition': 'Annual household income'},\n",
        "    {'Code': 'INDFMIN2', 'Definition': 'Annual family income'},\n",
        "    {'Code': 'INDFMPIR', 'Definition': 'Family income-to-poverty ratio'}\n",
        "]\n",
        "\n",
        "code_df = pd.DataFrame(code_data)"
      ],
      "metadata": {
        "id": "WPMsJ2IJ0yx0"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the file:"
      ],
      "metadata": {
        "id": "zg9S_uCm0tSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'biased_demographic.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Successfully loaded the data.\")\n",
        "print(\"\\n--- First 5 rows of the DataFrame ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- DataFrame Info (Column names, data types, non-null counts) ---\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTC1SEr9o8Ym",
        "outputId": "c6214124-b846-4c61-e44f-ee1835ddfc6f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded the data.\n",
            "\n",
            "--- First 5 rows of the DataFrame ---\n",
            "    SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDRETH1  \\\n",
            "0  73557         8         2         1        69       NaN         4   \n",
            "1  73558         8         2         1        54       NaN         3   \n",
            "2  73559         8         2         1        72       NaN         3   \n",
            "3  73560         8         2         1         9       NaN         3   \n",
            "4  73561         8         2         2        73       NaN         3   \n",
            "\n",
            "   RIDRETH3  RIDEXMON  RIDEXAGM  ...  DMDHREDU  DMDHRMAR  DMDHSEDU  \\\n",
            "0         4       1.0       NaN  ...       3.0       4.0       NaN   \n",
            "1         3       1.0       NaN  ...       3.0       1.0       1.0   \n",
            "2         3       2.0       NaN  ...       4.0       1.0       3.0   \n",
            "3         3       1.0     119.0  ...       3.0       1.0       4.0   \n",
            "4         3       1.0       NaN  ...       5.0       1.0       5.0   \n",
            "\n",
            "       WTINT2YR      WTMEC2YR  SDMVPSU  SDMVSTRA  INDHHIN2  INDFMIN2  INDFMPIR  \n",
            "0  13281.237386  13481.042095        1       112       4.0       4.0      0.84  \n",
            "1  23682.057386  24471.769625        1       108       7.0       7.0      1.78  \n",
            "2  57214.803319  57193.285376        1       109      10.0      10.0      4.51  \n",
            "3  55201.178592  55766.512438        2       109       9.0       9.0      2.52  \n",
            "4  63709.667069  65541.871229        2       116      15.0      15.0      5.00  \n",
            "\n",
            "[5 rows x 47 columns]\n",
            "\n",
            "--- DataFrame Info (Column names, data types, non-null counts) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10175 entries, 0 to 10174\n",
            "Data columns (total 47 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   SEQN      10175 non-null  int64  \n",
            " 1   SDDSRVYR  10175 non-null  int64  \n",
            " 2   RIDSTATR  10175 non-null  int64  \n",
            " 3   RIAGENDR  10175 non-null  int64  \n",
            " 4   RIDAGEYR  10175 non-null  int64  \n",
            " 5   RIDAGEMN  673 non-null    float64\n",
            " 6   RIDRETH1  10175 non-null  int64  \n",
            " 7   RIDRETH3  10175 non-null  int64  \n",
            " 8   RIDEXMON  9813 non-null   float64\n",
            " 9   RIDEXAGM  4213 non-null   float64\n",
            " 10  DMQMILIZ  6261 non-null   float64\n",
            " 11  DMQADFC   543 non-null    float64\n",
            " 12  DMDBORN4  10175 non-null  int64  \n",
            " 13  DMDCITZN  10171 non-null  float64\n",
            " 14  DMDYRSUS  1908 non-null   float64\n",
            " 15  DMDEDUC3  2803 non-null   float64\n",
            " 16  DMDEDUC2  5769 non-null   float64\n",
            " 17  DMDMARTL  5769 non-null   float64\n",
            " 18  RIDEXPRG  1309 non-null   float64\n",
            " 19  SIALANG   10175 non-null  int64  \n",
            " 20  SIAPROXY  10174 non-null  float64\n",
            " 21  SIAINTRP  10175 non-null  int64  \n",
            " 22  FIALANG   10054 non-null  float64\n",
            " 23  FIAPROXY  10054 non-null  float64\n",
            " 24  FIAINTRP  10054 non-null  float64\n",
            " 25  MIALANG   7311 non-null   float64\n",
            " 26  MIAPROXY  7312 non-null   float64\n",
            " 27  MIAINTRP  7313 non-null   float64\n",
            " 28  AIALANGA  6317 non-null   float64\n",
            " 29  DMDHHSIZ  10175 non-null  int64  \n",
            " 30  DMDFMSIZ  10175 non-null  int64  \n",
            " 31  DMDHHSZA  10175 non-null  int64  \n",
            " 32  DMDHHSZB  10175 non-null  int64  \n",
            " 33  DMDHHSZE  10175 non-null  int64  \n",
            " 34  DMDHRGND  10175 non-null  int64  \n",
            " 35  DMDHRAGE  10175 non-null  int64  \n",
            " 36  DMDHRBR4  9878 non-null   float64\n",
            " 37  DMDHREDU  9881 non-null   float64\n",
            " 38  DMDHRMAR  10052 non-null  float64\n",
            " 39  DMDHSEDU  5342 non-null   float64\n",
            " 40  WTINT2YR  10175 non-null  float64\n",
            " 41  WTMEC2YR  10175 non-null  float64\n",
            " 42  SDMVPSU   10175 non-null  int64  \n",
            " 43  SDMVSTRA  10175 non-null  int64  \n",
            " 44  INDHHIN2  10042 non-null  float64\n",
            " 45  INDFMIN2  10052 non-null  float64\n",
            " 46  INDFMPIR  9390 non-null   float64\n",
            "dtypes: float64(28), int64(19)\n",
            "memory usage: 3.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "Tk_4W6PVickB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are selecting the desired features from the dataset for the predictive model and cleaning the data."
      ],
      "metadata": {
        "id": "8VxRflnfinWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the features I am using for the predictive model\n",
        "predictive_features = [\n",
        "    \"RIAGENDR\",   # Gender of the participant\n",
        "    \"RIDRETH1\",   # Race/Hispanic origin (Recode)\n",
        "    \"DMDCITZN\",   # Citizenship status\n",
        "    \"DMDHHSIZ\",   # Total number of people in the household\n",
        "    \"DMDEDUC2\",   # Education level for participants aged 20 years and older\n",
        "    \"DMDMARTL\",   # Marital status\n",
        "    \"INDHHIN2\",   # Annual household income\n",
        "    \"DMDBORN4\",   # Country of Birth\n",
        "    \"SIALANG\"     # Language of the Sample Person interview\n",
        "]\n",
        "\n",
        "# Target feature: Total number of people in the household\n",
        "target_feature = \"DMDHHSIZ\"\n",
        "# We are seeking a binary prediction to establish if the total number of people in the household is greater than 2 based on the data (0 for x<2, 1 for x>2)\n",
        "\n",
        "# This feature requires cleaning since the selected options were improperly categorized which causes prediction issues.\n",
        "# \"INDHHIN2\",   # Annual household income\n"
      ],
      "metadata": {
        "id": "h74sRAYzig01"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are selecting the desired features from the dataset for the predictive model.\n",
        "\n",
        "Additionally, we are performing a cleaning step on the 'INDHHIN2' (Annual household income) feature. Based on our previous analysis and the data dictionary ([https://wwwn.cdc.gov/nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.htm#INDFMIN2](https://wwwn.cdc.gov/nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.htm#INDFMIN2)), codes 12 and 13 represent \"Under 20,000\" and \"Under 20,000\", respectively, which are irregular categorizations that overlap. To improve the accuracy and clarity of this feature for modeling, we are removing rows where 'INDHHIN2' has these values.\n",
        "\n",
        "The remaining features are relatively clean, but there is a notable amount of missing data in the 'DMDEDUC2' (Education level for participants aged 20 years and older) and 'DMDMARTL' (Marital status) categories. These missing values will need to be handled through imputation, as addressed in a later step of this notebook."
      ],
      "metadata": {
        "id": "tN2Ky0F2r3IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_data = df[predictive_features].copy()\n",
        "\n",
        "# Remove rows where 'INDHHIN2' is 12 or 13. According to the data dictionary, these codes\n",
        "# represent irregular categorizations that may overlap with other features, potentially\n",
        "# causing prediction issues. Removing them aims to clean the data.\n",
        "# https://wwwn.cdc.gov/nchs/Data/Nhanes/Public/2013/DataFiles/DEMO_H.htm#INDFMIN2\n",
        "selected_data = selected_data[~selected_data['INDHHIN2'].isin([12.0, 13.0])].copy()\n",
        "\n",
        "print(\"\\n--- Features selected for prediction ---\")\n",
        "print(selected_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwDRyaz_rU5t",
        "outputId": "090b042a-e995-442c-8ed2-43c610070949"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Features selected for prediction ---\n",
            "   RIAGENDR  RIDRETH1  DMDCITZN  DMDHHSIZ  DMDEDUC2  DMDMARTL  INDHHIN2  \\\n",
            "0         1         4       1.0         3       3.0       4.0       4.0   \n",
            "1         1         3       1.0         4       3.0       1.0       7.0   \n",
            "2         1         3       1.0         2       4.0       1.0      10.0   \n",
            "3         1         3       1.0         4       NaN       NaN       9.0   \n",
            "4         2         3       1.0         2       5.0       1.0      15.0   \n",
            "\n",
            "   DMDBORN4  SIALANG  \n",
            "0         1        1  \n",
            "1         1        1  \n",
            "2         1        1  \n",
            "3         1        1  \n",
            "4         1        1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am copying the dataset and checking how much missing data is present before the imputation."
      ],
      "metadata": {
        "id": "hNfbb3hWPM-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy dataset so we can work without changing the main data object which is used in the final pipeline\n",
        "final_data = selected_data.copy();\n",
        "print(\"Missing values between imputation:\")\n",
        "print(final_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n-ZGC_vPAwO",
        "outputId": "03e93752-11a0-4606-e40f-07da973aea41"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values between imputation:\n",
            "RIAGENDR       0\n",
            "RIDRETH1       0\n",
            "DMDCITZN       4\n",
            "DMDHHSIZ       0\n",
            "DMDEDUC2    4218\n",
            "DMDMARTL    4218\n",
            "INDHHIN2     133\n",
            "DMDBORN4       0\n",
            "SIALANG        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are missing values, I am imputing the data to handle the NaNs in the data to since the model fails with undefined values. We are avoiding imputing missing values for the target feature since we do not need fake values in the target column. Since we are working with mostly categorical data, we are using a simple imputer strategy of Most Frequent to address the missing values."
      ],
      "metadata": {
        "id": "WcRgKGfwPsEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values before splitting the data\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Identify features to impute (all columns except the target)\n",
        "features_to_impute = [col for col in final_data.columns if col != target_feature]\n",
        "\n",
        "# Apply imputation to the selected features\n",
        "final_data[features_to_impute] = imputer.fit_transform(final_data[features_to_impute])\n",
        "\n",
        "print(\"Missing values after imputation:\")\n",
        "print(final_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYBPQ6TP12XG",
        "outputId": "d8c43959-760b-49f9-efce-6466fa045c85"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            "RIAGENDR    0\n",
            "RIDRETH1    0\n",
            "DMDCITZN    0\n",
            "DMDHHSIZ    0\n",
            "DMDEDUC2    0\n",
            "DMDMARTL    0\n",
            "INDHHIN2    0\n",
            "DMDBORN4    0\n",
            "SIALANG     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Predictive Model"
      ],
      "metadata": {
        "id": "z8vEC9eQhP9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am setting up a predictive model pipeline for a random forest model."
      ],
      "metadata": {
        "id": "b_i6b908hUDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this prepares the data for the model,\n",
        "# X represents the columns without the target column\n",
        "# Y represents the target column and it gets converted into a binary value here.\n",
        "# Since we are trying to predict for households greater than 2 we have set the threshold to 2\n",
        "def prepare_data(df, target_column, threshold=2, test_size=0.2, random_state=42):\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = (pd.to_numeric(df[target_column], errors='coerce') > threshold).astype(int)\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "\n",
        "# Creates a train/test split for data pipeline and drops the target_feature column from the X\n",
        "(X_train, X_test, y_train, y_test) = prepare_data(final_data, target_feature)\n",
        "\n",
        "def train_pipeline(X_train, y_train, X_test, y_test):\n",
        "    # All features except the target are categorical\n",
        "    categorical_features = X_train.columns.tolist()\n",
        "    numerical_features = [] # No numerical features to scale\n",
        "\n",
        "    # Addressing class imbalance with RandomOverSampler, which is suitable for categorical data\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "    # This is setting up the preprocessor which is handling some additional imputation for missing values and scaling the features to reduce\n",
        "    # the impact of different scales and prepare the data for the model.\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', Pipeline([\n",
        "                ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values with the most frequent\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical features\n",
        "            ]), categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough' # Keep any other columns (none in this case)\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "randomForestPipeline = train_pipeline(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "tfyBfaokhgd0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model"
      ],
      "metadata": {
        "id": "ViHLfWNMqtiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = randomForestPipeline.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Test Results from RandomForestPipeline ---\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBWhBqJpqsOQ",
        "outputId": "dabe40fa-efd5-40bf-bf6f-9e8b78805110"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Results from RandomForestPipeline ---\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.62      0.55       501\n",
            "           1       0.86      0.78      0.81      1443\n",
            "\n",
            "    accuracy                           0.74      1944\n",
            "   macro avg       0.67      0.70      0.68      1944\n",
            "weighted avg       0.76      0.74      0.75      1944\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 313  188]\n",
            " [ 323 1120]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3fb9f7d"
      },
      "source": [
        "In this confusion matrix,\n",
        "\n",
        "*   True Negatives (313): The model correctly predicted that the household had 2 or fewer people.\n",
        "*   False Positives (188): The model incorrectly predicted that the household had more than 2 people when they actually had 2 or fewer people.\n",
        "*   False Negatives (323): The model incorrectly predicted that the household had 2 or fewer people when they actually had more than 2 people.\n",
        "*   True Positives (1120): The model correctly predicted that the household had more than 2 people."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Analysis"
      ],
      "metadata": {
        "id": "cJd5p682hnrf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a559b9b"
      },
      "source": [
        "The model performs significantly better at predicting Class 1 (households > 2 people) than Class 0 (households <= 2 people), as indicated by the higher precision, recall, and F1-score for Class 1. Quantitatively, the F1-score for Class 1 is 0.81, compared to 0.53 for Class 0, highlighting a substantial difference in the model's ability to correctly identify instances of each class while balancing precision and recall.\n",
        "\n",
        "The lower precision for Class 0 suggests that when the model predicts a household has 2 or fewer people, it is often wrong, misclassifying households that actually have more than 2 people.\n",
        "\n",
        "The class imbalance (1443 instances of Class 1 vs. 501 instances of Class 0) likely contributes to the model's better performance on the majority class (Class 1). RandomOverSampler was used to mitigate this, and while it helped improve the recall for Class 0 compared to a model without handling imbalance, there's still a notable difference in performance between the classes.\n",
        "\n",
        "The overall accuracy of 0.74 seems reasonable, but the weighted averages of precision, recall, and F1-score (around 0.74-0.76) provide a more balanced view of the model's performance across both classes, taking into account the imbalance. They indicate that the model's performance on the minority class is bringing down the overall weighted metrics compared to its performance on the majority class. This means that model performance can improve with more data that is featuring households that are less than 2 people.\n",
        "\n",
        "**Impact of Data Bias:**\n",
        "\n",
        "This dataset from a U.S. national survey (NHANES) likely contains inherent biases reflecting the survey's sampling methodology and the population it aims to represent. There appears to be an overrepresentation of data points corresponding to larger American families leading to a notable class imbalance where smaller households (Class 0) are the minority. Conversely, there might be insufficient representation of smaller families and potentially non-American individuals.\n",
        "\n",
        "This uneven data distribution significantly impacts the model's performance. The model demonstrates higher proficiency in identifying patterns within the overrepresented group (larger households) due to having more data to learn from. Consequently, it is less accurate in predicting for the underrepresented group (smaller households), contributing to the observed differences in precision, recall, and F1-score between the classes.\n",
        "\n",
        "Furthermore, features like 'DMDEDUC2' (Education level) and 'DMDMARTL' (Marital status) had a substantial amount of missing data. While 'most_frequent' imputation was used to handle these missing values, it's possible that this strategy had created problems of its own. This imputation strategy might reinforce patterns present in the majority of the data, potentially masking variations or unique characteristics within underrepresented subgroups. This could affect how the model learns from these features and potentially influence the fairness or accuracy of predictions for different demographic segments.\n",
        "\n",
        "In order to improve model performance, it will be beneficial to add more features to the data and reduce the missing data."
      ]
    }
  ]
}